{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages \n",
    "import pandas as pd\n",
    "\n",
    "# Import Python driver of Cassandra \n",
    "import cassandra\n",
    "\n",
    "# A regular expression (or RE) specifies a set of strings that matches it; \n",
    "# the functions in this module let you check if a particular string matches a \n",
    "# given regular expression (or if a given regular expression matches a particular \n",
    "# string, which comes down to the same thing).\n",
    "import re\n",
    "\n",
    "# The OS module in Python provides functions for interacting with the operating system. \n",
    "# OS comes under Python's standard utility modules. This module provides a portable way of \n",
    "# using operating system-dependent functionality\n",
    "import os\n",
    "\n",
    "# The glob module finds all the pathnames matching a specified pattern according to \n",
    "# the rules used by the Unix shell, although results are returned in arbitrary order. \n",
    "# No tilde expansion is done, but *, ?, and character ranges expressed with [] will be correctly matched.\n",
    "import glob\n",
    "\n",
    "# NumPy offers comprehensive mathematical functions, random number generators, \n",
    "# linear algebra routines, Fourier transforms, and more\n",
    "import numpy as np\n",
    "\n",
    "# While the JSON module will convert strings to Python datatypes, \n",
    "# normally the JSON functions are used to read and write directly from JSON files.\n",
    "import json\n",
    "\n",
    "# The so-called CSV (Comma Separated Values) format is the most common import \n",
    "# and export format for spreadsheets and databases.\n",
    "# The csv module implements classes to read and write tabular data in CSV format.\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Repositories\\public\\ETL-pipeline-and NoSQL-data-modeling-with-Apache-Cassandra\n"
     ]
    }
   ],
   "source": [
    "# checking your current working directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Repositories\\public\\ETL-pipeline-and NoSQL-data-modeling-with-Apache-Cassandra/event_data\n"
     ]
    }
   ],
   "source": [
    "# Get your current folder and subfolder event data\n",
    "filepath = os.getcwd() + '/event_data'\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a for loop to create a list of files and collect each filepath\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    \n",
    "# join the file path and roots with the subdirectories using glob\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))\n",
    "\n",
    "#print(file_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of CSV files =  30\n"
     ]
    }
   ],
   "source": [
    "# Files count from the previous list\n",
    "print(\"Count of CSV files = \", len(file_path_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correct! We have 30 CSV files in the folder '/event_data'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating an empty list of rows that will be generated from each file\n",
    "# We will collect all the rows in all data files inside this list\n",
    "full_data_rows_list = [] \n",
    "    \n",
    "# for every filepath in the file path list \n",
    "for file_path in file_path_list:\n",
    "\n",
    "    # reading csv file \n",
    "    with open(file_path, 'r', encoding = 'utf8', newline='') as csvfile:\n",
    "        \n",
    "        # creating a csv reader object \n",
    "        csvreader = csv.reader(csvfile)\n",
    "        \n",
    "        # next() method returns the current row and advances the iterator \n",
    "        # to the next row. the first row of our csv file contains the \n",
    "        # headers (or field names).\n",
    "        # Here, we don't return the first row/header, as we want\n",
    "        # to load the data rows only.\n",
    "        # And we will enter the columns header later on in the pipeline\n",
    "        next(csvreader)\n",
    "        \n",
    "        # extracting each data row one by one and append it \n",
    "        # Note that each row is inserted as a list\n",
    "        # That mean, at the end, we will have a BIG LIST OF LISTS == full_data_rows_list\n",
    "        for line in csvreader:\n",
    "            #print(line)\n",
    "            full_data_rows_list.append(line)\n",
    "        # By now we have inserted each row in the current file into the full_data_rows_list\n",
    "    # Current file is closed now, and we are ready to loop to the next file/file path\n",
    "\n",
    "# By now, all rows in all CSV files have been insterted into full_data_rows_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8056\n"
     ]
    }
   ],
   "source": [
    "# get total number of rows that are in the full_data_rows_list\n",
    "print(len(full_data_rows_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what the list of event data rows looks like\n",
    "#print(full_data_rows_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A dialect object or (simply dialect) is a way to group various formatting parameters. \n",
    "Once you have created the dialect object, simply pass it to the reader or writer, \n",
    "rather than passing each formatting argument separately.**\n",
    "\n",
    "**To create a new dialect, we use register_dialect() function. It accepts dialect name as a string and one or more formatting parameters as keyword arguments.**\n",
    "* `quoting`: controls when quotes should be generated by the writer or recognized by the reader (see above for other options). If you want double quotes around all fields regardless of whether quotechar or delimiter appears in the data or not, set quoting to csv.QUOTE_ALL.\n",
    "* `skipinitialspace`: It controls how the space following the delimiter will be interpreted. If True, the initial whitespaces will be removed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a smaller event data csv file called event_datafile_full csv that will be used to insert data into the \\\n",
    "# Apache Cassandra tables\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as event_datafile_new:\n",
    "    \n",
    "    # Create a new CSV file with the configurations we've specified in the dialect\n",
    "    writer = csv.writer(event_datafile_new, dialect='myDialect')\n",
    "    \n",
    "    # Now we've created the file, we want to insert the columns names/header row.\n",
    "    # and we want to do that before inserting the rows of data. Why?\n",
    "    # Because we have inserted the data rows only in the full_data_rows_list, and in each file\n",
    "    # we skipped the first row/header row.\n",
    "    # So, we enter the columns names manually here before loading our data\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userId'])\n",
    "    \n",
    "    # Now, we want to load our data. We loop over the rows, row by row,\n",
    "    for row in full_data_rows_list:\n",
    "        \n",
    "        # Check if the first element in the row is an empty string;\n",
    "        # that means the first field in that row is missing,\n",
    "        # and for the sake of this project, we will skip such entries.\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        \n",
    "        # Since the program reached here, it means that first field exists and we will write the entry\n",
    "        # in the 'event_datafile_new.csv' file.\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821\n"
     ]
    }
   ],
   "source": [
    "# check the number of rows in your csv file\n",
    "with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "    print(sum(1 for line in f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. The Apache Cassandra coding portion of the project. \n",
    "\n",
    "## Now you are ready to work with the CSV file titled <font color=red>event_datafile_new.csv</font>.  The event_datafile_new.csv contains the following columns: \n",
    "- 1. artist \n",
    "- 2. firstName of user\n",
    "- 3. gender of user\n",
    "- 4. item number in session\n",
    "- 5. last name of user\n",
    "- 6. length of the song\n",
    "- 7. level (paid or free song)\n",
    "- 8. location of the user\n",
    "- 9. sessionId\n",
    "- 10. song title\n",
    "- 11. userId\n",
    "\n",
    "<img src=\"images/image_event_datafile_new.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "# Create a connection the database\n",
    "# We will use local IP address; since we have a locally installed Apache cassandra instance\n",
    "cluster = Cluster(['127.0.0.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a session to execute inside it our queries\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x22268393bb0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A keyspace is the top-level database object \n",
    "# that controls the replication for the object \n",
    "# it contains at each datacenter in the cluster.\n",
    "\n",
    "# Keyspaces contain tables, materialized views and user-defined types, \n",
    "# functions and aggregates. \n",
    "# Typically, a cluster has one keyspace per application.\n",
    "\n",
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS udacity\n",
    "    WITH REPLICATION = \n",
    "        {'class' : 'SimpleStrategy', 'replication_factor' : 1}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set KEYSPACE to the keyspace specified above\n",
    "session.set_keyspace('udacity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to create tables to run the following queries. Remember, with Apache Cassandra you model the database tables on the queries you want to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create queries to ask the following three questions of the data\n",
    "\n",
    "### 1. Give me the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n",
    "`SELECT artist_name, song_title, song_length FROM session_songs WHERE session_id=338 AND item_in_session=4`\n",
    "\n",
    "### 2. Give me only the following: name of artist, song (sorted by itemInSession) and user (first and last name) for userid = 10, sessionid = 182\n",
    "`SELECT artist_name, song_title, user_first_name, user_last_name FROM user_sessions WHERE user_id=10 AND session_id=182 ORDER BY item_in_session`    \n",
    "\n",
    "### 3. Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "`SELECT user_first_name, user_last_name WHERE song_title='All Hands Against His Own'`\n",
    "\n",
    "<img src=\"images/keyspace-diagram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important notes regarding the Keyspace design:\n",
    "* We know that cassandra doesn't allow for duplicated rows\n",
    "* That's why I added the `user_id` column in the song_fans table; because user's first and last name, and the song title, can easily get repeated when two customers have the same name and listen to the same song.\n",
    "* In that case, Cassandra will overwrite the old row with the new row.\n",
    "* By adding the `user_id` field, we ensure that all users who listen to any song, will be in that table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table `session_songs`\n",
    "\n",
    "`SELECT artist_name, song_title, song_length FROM session_songs WHERE session_id=338 AND item_in_session=4`\n",
    "\n",
    "> for the `PRIMARY KEY`, since we need to filter the results by `session_id`, I will choose that column to be the `Partition Key`. Also, we want to filter the results by `item_in_session` as well, so, I will choose this to be a `Cluster Column`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x222683ad940>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the query to create the table\n",
    "query = \"CREATE TABLE IF NOT EXISTS session_songs \"\n",
    "query += \"(session_id INT, item_in_session INT, artist_name VARCHAR, \"\n",
    "query += \"song_title VARCHAR, song_length DECIMAL, \"\n",
    "query += \"PRIMARY KEY (session_id, item_in_session))\"\n",
    "\n",
    "# Execute the query and create the table\n",
    "session.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now, we want to load the data from `event_datafile_new` into our table\n",
    "\n",
    "# Set the INSERT query \n",
    "query = \"INSERT INTO session_songs \"\n",
    "query += \"(session_id, item_in_session, artist_name, song_title, song_length) \"\n",
    "query = query + \"VALUES (%s, %s, %s, %s, %s)\"\n",
    "\n",
    "# Set file name\n",
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "# open the CSV file\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    \n",
    "    # Read the CSV file\n",
    "    csvreader = csv.reader(f)\n",
    "    \n",
    "    # Skip first row/columns names/header row\n",
    "    next(csvreader) # skip header\n",
    "    \n",
    "    # Loop over the rows, row by row\n",
    "    for row in csvreader:\n",
    "\n",
    "        # We have 11 columns in each row in the 'event_datafile_new.csv' file\n",
    "        # Each row is basically a list\n",
    "        # To load the needed columns into session_songs table, we have to pick these columns\n",
    "        # from this list as follows:\n",
    "        # session_id: row[8]\n",
    "        # item_in_session: row[3]\n",
    "        # artist_name: row[0]\n",
    "        # song_title: row[9]\n",
    "        # song_length: row[5]\n",
    "        # CAREFULL: each element in each row is STRING. So, we need to convert each element\n",
    "        # to the appropriate data type\n",
    "        session.execute(query, (int(row[8]), int(row[3]), row[0], row[9], float(row[5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a SELECT to verify that the data have been inserted into each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(artist_name='Faithless', song_title='Music Matters (Mark Knight Dub)', song_length=Decimal('495.3073'))\n"
     ]
    }
   ],
   "source": [
    "# Give me the artist, song title and song's length in the music app history that was heard during \n",
    "# sessionId = 338, and itemInSession = 4\n",
    "\n",
    "# Set SELECT statement\n",
    "query = \"SELECT artist_name, song_title, song_length FROM session_songs \"\n",
    "query += \"WHERE session_id=338 AND item_in_session=4\"\n",
    "\n",
    "# Execute the query\n",
    "rows =session.execute(query)\n",
    "\n",
    "# Print the results\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table `user_sessions`\n",
    "\n",
    "`SELECT artist_name, song_title, user_first_name, user_last_name FROM user_sessions WHERE user_id=10 AND session_id=182 ORDER BY item_in_session` \n",
    "\n",
    "> for the `PRIMARY KEY`, since we need to filter the results by `user_id`, I will choose that column to be the `Partition Key`. Also, we want to filter the results by `session_id` as well, so, I will choose this to be a `Cluster Column`. Finally, we want to sort the results by `item_in_session`, so we set this as `Cluster Column` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x222684056d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the query to create the table\n",
    "query = \"CREATE TABLE IF NOT EXISTS user_sessions \"\n",
    "query += \"(user_id INT, session_id INT, item_in_session INT, \"\n",
    "query += \"user_first_name VARCHAR, user_last_name VARCHAR, \"\n",
    "query += \"artist_name VARCHAR, song_title VARCHAR, \"\n",
    "query += \"PRIMARY KEY (user_id, session_id, item_in_session))\"\n",
    "\n",
    "# Execute the query and create the table\n",
    "session.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now, we want to load the data from `event_datafile_new` into our table\n",
    "\n",
    "# Set the INSERT query \n",
    "query = \"INSERT INTO user_sessions \"\n",
    "query += \"(user_id, session_id, item_in_session, user_first_name, \"\n",
    "query += \"user_last_name, artist_name, song_title)\"\n",
    "query = query + \"VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n",
    "\n",
    "# Set file name\n",
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "# open the CSV file\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    \n",
    "    # Read the CSV file\n",
    "    csvreader = csv.reader(f)\n",
    "    \n",
    "    # Skip first row/columns names/header row\n",
    "    next(csvreader) # skip header\n",
    "    \n",
    "    # Loop over the rows, row by row\n",
    "    for row in csvreader:\n",
    "\n",
    "        # We have 11 columns in each row in the 'event_datafile_new.csv' file\n",
    "        # Each row is basically a list\n",
    "        # To load the needed columns into user_sessions table, we have to pick these columns\n",
    "        # from this list as follows:\n",
    "        # user_id: row[10]\n",
    "        # session_id: row[8]\n",
    "        # item_in_session: row[3]\n",
    "        # user_first_name: row[1]\n",
    "        # user_last_name: row[4]\n",
    "        # artist_name: row[0]\n",
    "        # song_title: row[9]\n",
    "        # CAREFULL: each element in each row is STRING. So, we need to convert each element\n",
    "        # to the appropriate data type\n",
    "        session.execute(query, (int(row[10]), int(row[8]), int(row[3]), row[1], row[4], row[0], row[9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a SELECT to verify that the data have been inserted into each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(item_in_session=0, artist_name='Down To The Bone', song_title=\"Keep On Keepin' On\", user_first_name='Sylvie', user_last_name='Cruz')\n",
      "\n",
      "Row(item_in_session=1, artist_name='Three Drives', song_title='Greece 2000', user_first_name='Sylvie', user_last_name='Cruz')\n",
      "\n",
      "Row(item_in_session=2, artist_name='Sebastien Tellier', song_title='Kilometer', user_first_name='Sylvie', user_last_name='Cruz')\n",
      "\n",
      "Row(item_in_session=3, artist_name='Lonnie Gordon', song_title='Catch You Baby (Steve Pitron & Max Sanna Radio Edit)', user_first_name='Sylvie', user_last_name='Cruz')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set SELECT statement\n",
    "query = \"SELECT item_in_session, artist_name, song_title, user_first_name, user_last_name \"\n",
    "query += \"FROM user_sessions \"\n",
    "query += \"WHERE user_id=10 AND session_id=182 \"\n",
    "query += \"ORDER BY session_id, item_in_session\"\n",
    "\n",
    "# Execute the query\n",
    "rows =session.execute(query)\n",
    "\n",
    "# Print the results\n",
    "for row in rows:\n",
    "    print(row, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table `song_fans`\n",
    "\n",
    "`SELECT user_first_name, user_last_name WHERE song_title='All Hands Against His Own'`\n",
    "\n",
    "> for the `PRIMARY KEY`, since we need to filter the results by `song_title`, I will choose that column to be the `Partition Key`. But, the song_title is not unique to that table! Multiple users can listen to the same song, and multiple users can have the same first and last name, so, I choose `user_id` to be a `Clustering Column`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x2226838d6a0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the query to create the table\n",
    "query = \"CREATE TABLE IF NOT EXISTS song_fans \"\n",
    "query += \"(user_id INT, song_title VARCHAR, user_first_name VARCHAR, user_last_name VARCHAR, \"\n",
    "query += \"PRIMARY KEY (song_title, user_id))\"\n",
    "\n",
    "# Execute the query and create the table\n",
    "session.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now, we want to load the data from `event_datafile_new` into our table\n",
    "\n",
    "# Set the INSERT query \n",
    "query = \"INSERT INTO song_fans \"\n",
    "query += \"(user_id, song_title, user_first_name, user_last_name)\"\n",
    "query = query + \"VALUES (%s, %s, %s, %s)\"\n",
    "\n",
    "# Set file name\n",
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "# open the CSV file\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    \n",
    "    # Read the CSV file\n",
    "    csvreader = csv.reader(f)\n",
    "    \n",
    "    # Skip first row/columns names/header row\n",
    "    next(csvreader) # skip header\n",
    "    \n",
    "    # Loop over the rows, row by row\n",
    "    for row in csvreader:\n",
    "\n",
    "        # We have 11 columns in each row in the 'event_datafile_new.csv' file\n",
    "        # Each row is basically a list\n",
    "        # To load the needed columns into song_fans table, we have to pick these columns\n",
    "        # from this list as follows:\n",
    "        # user_id: row[10]\n",
    "        # song_title: row[9]\n",
    "        # user_first_name: row[1]\n",
    "        # user_last_name: row[4]\n",
    "        # CAREFULL: each element in each row is STRING. So, we need to convert each element\n",
    "        # to the appropriate data type\n",
    "        session.execute(query, (int(row[10]), row[9], row[1], row[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a SELECT to verify that the data have been inserted into each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(user_first_name='Jacqueline', user_last_name='Lynch')\n",
      "\n",
      "Row(user_first_name='Tegan', user_last_name='Levine')\n",
      "\n",
      "Row(user_first_name='Sara', user_last_name='Johnson')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Give me the artist, song title and song's length in the music app history that was heard during \n",
    "# sessionId = 338, and itemInSession = 4\n",
    "\n",
    "# Set SELECT statement\n",
    "query = \"SELECT user_first_name, user_last_name \"\n",
    "query += \"FROM song_fans WHERE song_title='All Hands Against His Own'\"\n",
    "\n",
    "# Execute the query\n",
    "rows =session.execute(query)\n",
    "\n",
    "# Print the results\n",
    "for row in rows:\n",
    "    print(row, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x222683adb50>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"DROP TABLE IF EXISTS session_songs\"\n",
    "session.execute(query)\n",
    "\n",
    "query = \"DROP TABLE IF EXISTS user_sessions\"\n",
    "session.execute(query)\n",
    "\n",
    "query = \"DROP TABLE IF EXISTS song_fans\"\n",
    "session.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x222684015e0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"DROP KEYSPACE IF EXISTS udacity\"\n",
    "session.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session and cluster connection¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
